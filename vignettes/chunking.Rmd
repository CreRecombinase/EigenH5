---
title: "Chunking in HDF5"
author: "Nicholas Knoblauch"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


```{r parameters,echo=F}
p <- 300000L
N <- 4000L

```


# Determining optimal storage for SNP data

We'll be assuming that we intend to read subsets of SNPs for all individuals, and that we have more SNPs than we have individuals.  For this example, we'll have `r p` SNPs and `r N` individuals.  Remember that all matrices in `R` are column major, and that `HDF5` stores data in a row major order.


## To Transpose Or Not To Transpose


We'll create two SNP matrices, one SNPs first, and one samples first.


```{r,eval=FALSE}
library(EigenH5)
tf <- tempfile()
CHUNK_MAX = 1024*1024
chunk_sizes <- c(N, 100L)
create_matrix_h5(filename = tf,groupname = "test_X",dataname = "test_T",dimensions = as.integer(c(N,p)),doTranspose = T,chunksizes = chunk_sizes)
create_matrix_h5(filename = tf,groupname = "test_X",dataname = "test_N",dimensions = as.integer(c(N,p)),doTranspose = F,chunksizes = chunk_sizes)
create_matrix_h5(filename = tf,groupname = "Ttest_X",dataname = "test_T",dimensions = as.integer(c(p,N)),doTranspose = T,chunksizes = chunk_sizes)
create_matrix_h5(filename = tf,groupname = "Ttest_X",dataname = "test_N",dimensions = as.integer(c(p,N)),doTranspose = F,chunksizes = chunk_sizes)
write_chunk_size <- chunk_sizes[2]*100
n_chunks <- ceiling(p/write_chunk_size)
chunkl <- split(1:p,cut(1:p,breaks = n_chunks,labels = F))
pb <- progress::progress_bar$new(total=length(chunkl))
t_tlw <- list()
t_tlr <- list()
t_nlr <- list()
t_nlw <- list()

n_tlw <- list()
n_tlr <- list()
n_nlr <- list()
n_nlw <- list()
tX <- matrix(0.1,3,3)
for(i in 1:length(chunkl)){
  pb$tick()
  tp <- length(chunkl[[i]])
  offset <- chunkl[[i]][1]-1
  n <- tp*N
  if(prod(dim(tX))!=n){
    tX <- matrix(as.numeric(rbinom(n=n,size = 2,prob = 0.01)),N,tp)
    ttX <- t(tX)
  }
  n_tlw[[i]] <- system.time(
    write_mat_chunk_h5(filename = tf,
                       groupname = "test_X",
                       dataname = "test_T",
                       data = tX,
                       offsets = as.integer(c(0,offset)),
                       chunksizes = as.integer(c(N,tp))))
  n_nlw[[i]] <- system.time(
    write_mat_chunk_h5(filename = tf,
                       groupname = "test_X",
                       dataname = "test_N",
                       data = tX,
                       offsets = as.integer(c(0,offset)),
                       chunksizes = as.integer(c(N,tp))))
  n_nlr[[i]] <- system.time(tX <-
                              read_mat_h5(filename = tf,
                                          groupname = "test_X",
                                          dataname = "test_N",
                                          offsets = as.integer(c(0,offset)),
                                          chunksizes = as.integer(c(N,tp))))
  n_tlr[[i]] <- system.time(tX <-
                              read_mat_h5(filename = tf,
                                          groupname = "test_X",
                                          dataname = "test_T",
                                          offsets = as.integer(c(0,offset)),
                                          chunksizes = as.integer(c(N,tp))))
  t_tlw[[i]] <- system.time(
    write_mat_chunk_h5(filename = tf,
                       groupname = "Ttest_X",
                       dataname = "test_T",
                       data = ttX,
                       offsets = as.integer(c(offset,0)),
                       chunksizes = as.integer(c(tp,N))))
  t_nlw[[i]] <- system.time(
    write_mat_chunk_h5(filename = tf,
                       groupname = "Ttest_X",
                       dataname = "test_N",
                       data = ttX,
                       offsets = as.integer(c(offset,0)),
                       chunksizes = as.integer(c(tp,N))))
  t_nlr[[i]] <- system.time(ttX <-
                              read_mat_h5(filename = tf,
                                          groupname = "Ttest_X",
                                          dataname = "test_N",
                                          offsets = as.integer(c(offset,0)),
                                          chunksizes = as.integer(c(tp,N))))
  t_tlr[[i]] <- system.time(ttX <-
                              read_mat_h5(filename = tf,
                                          groupname = "Ttest_X",
                                          dataname = "test_T",
                                          offsets = as.integer(c(offset,0)),
                                          chunksizes = as.integer(c(tp,N))))
  
}



```
```{r,eval=FALSE}
summary_time <- data_frame(t_t_read=map_dbl(t_tlr,~.x[3]),
                           t_t_write=map_dbl(t_tlw,~.x[3]),
                           t_n_read=map_dbl(t_nlr,~.x[3]),
                           t_n_write=map_dbl(t_nlw,~.x[3]),
                           n_t_read=map_dbl(n_tlr,~.x[3]),
                           n_t_write=map_dbl(n_tlw,~.x[3]),
                           n_n_read=map_dbl(n_nlr,~.x[3]),
                           n_n_write=map_dbl(n_nlw,~.x[3]),
                           chunk_size=lengths(chunkl),
                           chunk=1:length(chunkl))
tidy_time <- gather(summary_time,class,time,-chunk_size,-chunk) %>% separate(class,c("SNP_first","transposed","read_write"),"_") %>% mutate(SNP_first=SNP_first=="t",transposed=transposed=="t")
tidy_time %>% ggplot(aes(x=transposed,y=time,col=SNP_first))+geom_point()+geom_jitter()+scale_y_log10()+facet_wrap(~read_write)
```


```{r,eval=FALSE}
filter(tidy_time,!transposed,read_write=="read") %>% rename(runtime=time) %>% ggplot(aes(x=SNP_first,y=runtime,col=SNP_first))+geom_point()+facet_wrap(~read_write)

```


#What about chunksize?


```{r,eval=FALSE}
library(EigenH5)
tf <- tempfile()

chunk_sizes <- c(N, 100L)
dims <- as.integer(c(N,p))
guess_chunks(dims)
chunk_size_p <- as.integer(10^(seq(0,4,length.out = 10)))
pb <- progress::progress_bar$new(total=length(chunk_size_p))
dfl <- list()
for(j in 1:length(chunk_size_p)){
  create_matrix_h5(filename = tf,
                   groupname = "test_X",
                   dataname = "test_N",
                   dimensions = as.integer(c(N,p)),
                   doTranspose = F,
                   chunksizes = c(N,chunk_size_p[j]))
  dfl[[j]] <-  rwf(tf,"test_X","test_N",as.integer(c(N,p)),tX,chunk_size_p[j])
  pb$tick()
  file.remove(tf)
}
rwf <- function(filename,groupname,dataname,data_dims,data_matrix,disk_chunk_size){
  tp <- ncol(tX)
  p <- data_dims[2]
  mb <- microbenchmark::microbenchmark(write=write_mat_chunk_h5(filename = filename,
                                                                       groupname = groupname,
                                                                       dataname = dataname,
                                                                       data = data_matrix,
                                                                       offsets = as.integer(c(0,sample(0:(p-tp),size = 1))),
                                                                       chunksizes = as.integer(c(N,tp))),
                                              read=read_mat_h5(filename = filename,
                                                               groupname = groupname,
                                                               dataname = dataname,
                                                               offsets = as.integer(c(0,sample(0:(p-tp),size = 1))),
                                                               chunksizes = as.integer(c(N,tp))),times=20L) %>% 
    as_data_frame() %>% 
    mutate(disk_chunk_size=disk_chunk_size)
  return(mb)
                                                
                                             
}
df_res <- bind_rows(dfl)
```

```{r,eval=FALSE}
ggplot(df_res,aes(x=disk_chunk_size/p,y=time))+geom_point()+facet_wrap(~expr)+scale_x_log10()
```









